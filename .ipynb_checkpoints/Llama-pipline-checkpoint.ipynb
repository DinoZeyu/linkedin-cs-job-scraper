{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528bde08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/zeyuhan/.conda/envs/lk/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e48c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face to use LLaMA model\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e081b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"linkedin_jobs.csv\") \n",
    "text_columns = [c for c in\n",
    "    [\"Description\",\"Responsibilities\",\"QualificationsRequired\",\"QualificationsPreferred\",\"Requirements\"]\n",
    "    if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc28a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set GPU and percision \n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\") \n",
    "\n",
    "# Use the LLama-3.1-8B-Instruct Model locally\n",
    "Model_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Model_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    Model_ID,\n",
    "    dtype=torch.bfloat16,   \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101091e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(field_name, field_text):\n",
    "    system = (\n",
    "        \"You extract technical computer science skills from job postings. \"\n",
    "        \"Return only the skills line; no explanations.\"\n",
    "    )\n",
    "    user = f\"\"\"Extract technical CS skills.\n",
    "\n",
    "Rules:\n",
    "- Include technical skills and years of experience.\n",
    "- Deduplicate terms: keep multi-word phrases intact.\n",
    "- Output EXACTLY one line: comma-separated list. \n",
    "\n",
    "Field: {field_name}\n",
    "Text:\n",
    "<<<\n",
    "{field_text}\n",
    ">>>\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ab73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_skills_line(text: str):\n",
    "    line = next((ln.strip() for ln in text.splitlines() if ln.strip()), \"\")\n",
    "    line = line.replace(\";\", \",\")\n",
    "    parts = [p.strip(\" .\\t\") for p in line.split(\",\") if p.strip()]\n",
    "    out, seen = [], set()\n",
    "    for p in parts:\n",
    "        k = p.lower()\n",
    "        if k and k != \"none\" and k not in seen:\n",
    "            seen.add(k); out.append(p)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a32a7fa-2c5c-420c-8348-29092d8c2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cs_terms_batched(df, text_columns, batch_size,\n",
    "                             max_input_tokens=2048, max_new_tokens=100):\n",
    "\n",
    "    # Extract the information of dataframes into huggingface standard prompt format\n",
    "    pairs = []\n",
    "    for i, row in df.iterrows():\n",
    "        for col in text_columns:\n",
    "            val = row.get(col, \"\")\n",
    "            val = \"\" if pd.isna(val) else str(val)\n",
    "            if val.strip() and val.lower() != \"nan\":\n",
    "                messages = make_prompt(col, val)           \n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                pairs.append((i, prompt))\n",
    "\n",
    "    # Parallel Processing\n",
    "    row_to_skills = defaultdict(set)\n",
    "    for k in tqdm(range(0, len(pairs), batch_size), desc=\"Extracting CS skills (batched)\"):\n",
    "        batch = pairs[k:k+batch_size]\n",
    "        idxs, prompts = zip(*batch)\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            list(prompts),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_tokens\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  \n",
    "                use_cache = False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        texts = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        for row_idx, text in zip(idxs, texts):\n",
    "            for s in _parse_skills_line(text):\n",
    "                row_to_skills[row_idx].add(s)\n",
    "\n",
    "    # Store the skills in one column with corresponding rows\n",
    "    return pd.Series(\n",
    "        [\", \".join(sorted(row_to_skills.get(i, set()), key=str.lower)) for i in df.index],\n",
    "        index=df.index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee671f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CS skills (batched):   0%|          | 0/604 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Extracting CS skills (batched):  12%|█▏        | 75/604 [30:37<2:29:08, 16.92s/it]"
     ]
    }
   ],
   "source": [
    "df[\"CS_Terms\"] = extract_cs_terms_batched(df, text_columns, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eba280-bb66-4cf2-9e20-fd2d0970e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Linkedin_Job_Requirements_LLama.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
