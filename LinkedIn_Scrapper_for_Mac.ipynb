{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a7973f",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacd2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/50357691/Desktop/Job_Skill_Gap_Analysis/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from math import ceil\n",
    "from typing import Optional\n",
    "import os, time, logging, pandas as pd, re\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException,\n",
    "    NoSuchWindowException, WebDriverException,\n",
    "    ElementClickInterceptedException, StaleElementReferenceException\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "class ExperienceFilter(Enum):\n",
    "    ALL = \"\"\n",
    "    INTERNSHIP = \"1\"\n",
    "    ENTRY_LEVEL = \"2\"\n",
    "    ASSOCIATE = \"3\"\n",
    "    MID_SENIOR_LEVEL = \"4\"\n",
    "    DIRECTOR = \"5\"\n",
    "    EXECUTIVE = \"6\"\n",
    "\n",
    "class JobPostTime(Enum):\n",
    "    ANY_TIME = \"\"\n",
    "    PAST_24_HOURS = \"r86400\"\n",
    "    PAST_WEEK = \"r604800\"\n",
    "    PAST_MONTH = \"r2592000\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1cc45",
   "metadata": {},
   "source": [
    "## Use chrome driver to search for job postinigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6a7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium is a automation tool that can control a web browser. It could do human actions in web pages like click, scroll, input text, etc. Which can be used to bypass some anti-scraping mechanisms.\n",
    "\n",
    "driver = None\n",
    "\n",
    "def make_driver(headless: bool = False):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--lang=en-US\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "def ensure_window(drv, headless: bool = False):\n",
    "    global driver\n",
    "    if drv is None:\n",
    "        driver = make_driver(headless=headless)\n",
    "        return driver\n",
    "    try:\n",
    "        _ = drv.current_url\n",
    "        if not drv.window_handles:\n",
    "            raise NoSuchWindowException(\"No window handles\")\n",
    "        return drv\n",
    "    except Exception:\n",
    "        logging.warning(\"Driver/window lost. Recreating browser...\")\n",
    "        try:\n",
    "            drv.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "        driver = make_driver(headless=headless)\n",
    "        return driver\n",
    "\n",
    "def safe_get(drv, url: str, retries: int = 1, headless: bool = False):\n",
    "    global driver\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            drv.get(url)\n",
    "            return drv\n",
    "        except WebDriverException as e:\n",
    "            logging.warning(f\"driver.get failed ({e.__class__.__name__}); recreating…\")\n",
    "            drv = ensure_window(drv, headless=headless)\n",
    "            driver = drv\n",
    "    return drv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5a46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "def try_login(drv, email: str, password: str) -> None:\n",
    "    global driver\n",
    "    driver = ensure_window(drv)\n",
    "    driver = safe_get(driver, \"https://www.linkedin.com/login\")\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "    u = driver.find_element(By.ID, \"username\"); u.clear(); u.send_keys(email)\n",
    "    p = driver.find_element(By.ID, \"password\"); p.clear(); p.send_keys(password); p.send_keys(Keys.RETURN)\n",
    "    WebDriverWait(driver, 25).until(\n",
    "        EC.any_of(\n",
    "            EC.presence_of_element_located((By.ID, \"global-nav-search\")),\n",
    "            EC.presence_of_element_located((By.ID, \"input__email_verification_pin\")),\n",
    "            EC.url_contains(\"checkpoint\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _maybe_accept_cookies_and_check_login(drv) -> bool:\n",
    "    for sel in [\n",
    "        \"button[aria-label*='Accept']\",\n",
    "        \"button[data-test-global-nav-cookie-banner-accept]\",\n",
    "        \".artdeco-global-alert-action button\",\n",
    "    ]:\n",
    "        try:\n",
    "            for b in drv.find_elements(By.CSS_SELECTOR, sel):\n",
    "                if b.is_displayed() and b.is_enabled():\n",
    "                    b.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "    cur = drv.current_url\n",
    "    if \"login\" in cur or \"checkpoint\" in cur:\n",
    "        return False\n",
    "    try:\n",
    "        WebDriverWait(drv, 6).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"main, body\")))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def login_wall_present(drv):\n",
    "    try:\n",
    "        modals = drv.find_elements(By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')\n",
    "        return any(m.is_displayed() for m in modals)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# LinkedIn has many different languages, so the dismiss button could be in different languages. Here we try to cover some common ones.\n",
    "def dismiss_login_wall(drv, timeout=3) -> bool:\n",
    "    selectors = [\n",
    "        'button.artdeco-modal__dismiss',\n",
    "        'button[aria-label*=\"Dismiss\"]',\n",
    "        'button[aria-label*=\"Close\"]',\n",
    "        'button[aria-label*=\"关闭\"]',\n",
    "        'button[aria-label*=\"關閉\"]',\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, timeout).until(EC.element_to_be_clickable((By.CSS_SELECTOR, sel)))\n",
    "            drv.execute_script(\"arguments[0].click();\", btn)\n",
    "            WebDriverWait(drv, 5).until_not(EC.presence_of_element_located((By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')))\n",
    "            return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        drv.find_element(By.TAG_NAME, \"body\").send_keys(Keys.ESCAPE)\n",
    "        WebDriverWait(drv, 2).until_not(EC.presence_of_element_located((By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Credentials \n",
    "EMAIL = \"zh272@georgetown.edu\"\n",
    "\n",
    "# Protect your password\n",
    "PASSWORD = os.getenv(\"LI_PASSWORD\") or getpass(\"Enter your LinkedIn password (hidden): \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec530c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS selectors for job listings container and items, which help to locate job postings on the page.\n",
    "CONTAINER_SELECTORS = [\n",
    "    \"div.jobs-search-results-list\",\n",
    "    \".scaffold-layout__list > div\",\n",
    "    \"div.jobs-search-two-pane__results\",\n",
    "    \"div.jobs-search-results-list__container\",\n",
    "    \"section.two-pane-serp-page__results-list\",\n",
    "]\n",
    "\n",
    "ITEM_SELECTORS = [\n",
    "    \"li[data-occludable-job-id]\",\n",
    "    \"ul.jobs-search-results__list > li\",\n",
    "    \"li.jobs-search-results__list-item\",\n",
    "    \"div.jobs-search-results__list-item\",\n",
    "    \"ul.jobs-search__results-list li\",\n",
    "    \"div.base-card, div.job-card-container\",\n",
    "]\n",
    "\n",
    "\n",
    "def wait_for_job_items(drv, timeout=20):\n",
    "    any_sel = \", \".join(ITEM_SELECTORS)\n",
    "    WebDriverWait(drv, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, any_sel)))\n",
    "\n",
    "def safe_text(el):\n",
    "    return el.text.strip() if el else \"\"\n",
    "\n",
    "def find_one(drv, *locator, timeout=8, visibility=True):\n",
    "    cond = EC.visibility_of_element_located if visibility else EC.presence_of_element_located\n",
    "    try:\n",
    "        return WebDriverWait(drv, timeout).until(cond(locator))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_title_company_location(drv):\n",
    "    # Title\n",
    "    title = \"\"\n",
    "    for sel in [\".job-view-layout h1\", \"h1.jobs-unified-top-card__job-title\", \"h1\"]:\n",
    "        els = drv.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els and els[0].text.strip():\n",
    "            title = els[0].text.strip(); break\n",
    "\n",
    "    # Company\n",
    "    company = \"\"\n",
    "    for sel in [\n",
    "        \"[data-view-name='job-details-about-company-name-link']\",\n",
    "        \"a.jobs-unified-top-card__company-name\",\n",
    "        \".jobs-unified-top-card__company-name a\",\n",
    "        \".jobs-unified-top-card__company-name\",\n",
    "    ]:\n",
    "        els = drv.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els and els[0].text.strip():\n",
    "            company = els[0].text.strip(); break\n",
    "\n",
    "    # Location: keep Remote/Hybrid/On-site and specific city/region. If it only has country, then keep empty\n",
    "    import re\n",
    "    location = \"\"\n",
    "    sel_list = [\n",
    "        \".jobs-unified-top-card__primary-description-container li\",\n",
    "        \".jobs-unified-top-card__primary-description-container span\",\n",
    "        \".job-details-jobs-unified-top-card__primary-description-container li\",\n",
    "        \".job-details-jobs-unified-top-card__primary-description-container span\",\n",
    "        \".jobs-unified-top-card__subtitle-primary-grouping span\",\n",
    "        \".jobs-unified-top-card__subtitle-primary-grouping\",\n",
    "        \".jobs-unified-top-card__bullet\",\n",
    "        \".jobs-unified-top-card__workplace-type\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for sel in sel_list:\n",
    "        for e in drv.find_elements(By.CSS_SELECTOR, sel):\n",
    "            t = e.text.strip()\n",
    "            if t:\n",
    "                candidates.append(t)\n",
    "\n",
    "    for t in candidates:\n",
    "        low = t.lower()\n",
    "        if any(k in low for k in [\"remote\", \"hybrid\", \"on-site\", \"onsite\"]):\n",
    "            location = t; break\n",
    "\n",
    "    if not location:\n",
    "        for t in candidates:\n",
    "            if re.search(r\".+,\\s+.+\", t):  \n",
    "                location = t; break\n",
    "\n",
    "    if location and location.strip().lower() in {\"united states\", \"usa\"}:\n",
    "        location = \"\"\n",
    "\n",
    "    return title, company, location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b3149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most job descriptions are partially hidden and need to be expanded by clicking \"See more\" or similar buttons. This function tries to find and click those buttons to reveal the full job description.\n",
    "def expand_all_in(drv, root=None, max_clicks=10):\n",
    "    selectors = [\n",
    "        \"button.show-more-less-html__button\",\n",
    "        \"button[aria-label*='See more']\",\n",
    "        \"button[aria-label*='Show more']\",\n",
    "        \"button[aria-controls*='description']\",\n",
    "        \"button[aria-expanded='false']\",\n",
    "        \"button[aria-label*='显示更多']\",\n",
    "        \"button[aria-label*='顯示更多']\",\n",
    "    ]\n",
    "    did = False\n",
    "    for _ in range(max_clicks):\n",
    "        clicked_this_round = False\n",
    "        scope = root if root is not None else drv\n",
    "        for sel in selectors:\n",
    "            for b in scope.find_elements(By.CSS_SELECTOR, sel):\n",
    "                try:\n",
    "                    if not b.is_displayed() or not b.is_enabled():\n",
    "                        continue\n",
    "                    drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                    time.sleep(0.15)\n",
    "                    drv.execute_script(\"arguments[0].click();\", b)\n",
    "                    did = True; clicked_this_round = True\n",
    "                    time.sleep(0.2)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if not clicked_this_round:\n",
    "            break\n",
    "    return did\n",
    "\n",
    "def scroll_until_all_jobs_load(drv, container, pause=0.8, max_tries=30, item_selector=\"li.jobs-search-results__list-item\"):\n",
    "    tries = 0\n",
    "    last_height = drv.execute_script(\"return arguments[0].scrollHeight\", container)\n",
    "    while tries < max_tries:\n",
    "        drv.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", container)\n",
    "        time.sleep(pause)\n",
    "        new_height = drv.execute_script(\"return arguments[0].scrollHeight\", container)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height; tries += 1\n",
    "    items = drv.find_elements(By.CSS_SELECTOR, item_selector)\n",
    "    if not items:\n",
    "        items = drv.find_elements(By.CSS_SELECTOR, \"ul.jobs-search-results__list > li, li[data-occludable-job-id], ul.jobs-search__results-list li\")\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c994c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_text(root, selectors):\n",
    "    for sel in selectors:\n",
    "        els = root.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            txt = els[0].text.strip()\n",
    "            if txt:\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "def _first_el(root, selectors):\n",
    "    for sel in selectors:\n",
    "        els = root.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            return els[0]\n",
    "    return None\n",
    "\n",
    "def _first_attr(root, selectors, attr):\n",
    "    el = _first_el(root, selectors)\n",
    "    return el.get_attribute(attr) if el else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the jobs data from the job listings\n",
    "def populate_jobs_data_minimal(jobs, nbOfJobs):\n",
    "    job_data = {\"Title\": [], \"Company\": [], \"Location\": [], \"Link\": [], \"Description\": []}\n",
    "    totalJobs = min(nbOfJobs, len(jobs))\n",
    "    list_handle = driver.current_window_handle\n",
    "\n",
    "    for idx, job in enumerate(jobs[:totalJobs]):\n",
    "        try:\n",
    "            link_el = _first_el(job, [\"a.base-card__full-link\", \"a.job-card-container__link\", \"a[href*='/jobs/view/']\"])\n",
    "            if not link_el: continue\n",
    "            link = link_el.get_attribute(\"href\") or \"\"\n",
    "            driver.execute_script(\"window.open(arguments[0], '_blank');\", link)\n",
    "            WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".jobs-unified-top-card, .job-view-layout, h1\")))\n",
    "            expand_all_in(driver, None, max_clicks=8)\n",
    "\n",
    "            title, company, location = get_title_company_location(driver)\n",
    "\n",
    "            desc_root = None\n",
    "            for sel in [\"#job-details\", \".jobs-description__content\", \".jobs-box__html-content\", \".show-more-less-html__markup\"]:\n",
    "                found = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "                if found: desc_root = found[0]; break\n",
    "            if desc_root: expand_all_in(driver, desc_root, max_clicks=5)\n",
    "\n",
    "            description = desc_root.text.strip() if desc_root else \"\"\n",
    "\n",
    "            job_data[\"Company\"].append(company)\n",
    "            job_data[\"Title\"].append(title)\n",
    "            job_data[\"Location\"].append(location)\n",
    "            job_data[\"Link\"].append(link)\n",
    "            job_data[\"Description\"].append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[details-minimal] job {idx} skipped: {e}\")\n",
    "        finally:\n",
    "            if len(driver.window_handles) > 1:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(list_handle)\n",
    "                if login_wall_present(driver):\n",
    "                    dismiss_login_wall(driver)\n",
    "\n",
    "    return pd.DataFrame(job_data)\n",
    "\n",
    "\n",
    "# read data from the job cards in the listing page, without visiting each job's detail page\n",
    "def extract_from_list_cards_min(max_items: int = 25) -> pd.DataFrame:\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
    "    if not cards:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search-results__list li\")\n",
    "    if not cards:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"div.base-card, div.job-card-container\")\n",
    "\n",
    "    data = {\"Title\": [], \"Company\": [], \"Location\": [], \"Link\": [], \"Description\": []}\n",
    "    for c in cards[:max_items]:\n",
    "        try:\n",
    "            title = _first_text(c, [\".base-search-card__title\", \".job-card-list__title\", \"h3\"])\n",
    "            company = _first_text(c, [\n",
    "                \".base-search-card__subtitle a\", \".base-search-card__subtitle\",\n",
    "                \".job-card-container__company-name\", \".job-search-card__subtitle a\",\n",
    "                \".job-search-card__subtitle\"\n",
    "            ])\n",
    "            location = _first_text(c, [\n",
    "                \".job-search-card__location\", \"span.job-search-card__location\",\n",
    "                \"li.job-card-container__metadata-item\"\n",
    "            ])\n",
    "            link = _first_attr(c, [\"a.base-card__full-link\", \"a.job-card-container__link\", \"a[href*='/jobs/view/']\"], \"href\")\n",
    "\n",
    "            if title or company or link:\n",
    "                data[\"Title\"].append(title); data[\"Company\"].append(company); data[\"Location\"].append(location)\n",
    "                data[\"Link\"].append(link); data[\"Description\"].append(\"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c057751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape jobs from the current page\n",
    "def scrape_jobs_from_page(nb_to_scrape: int) -> pd.DataFrame:\n",
    "    if login_wall_present(driver):\n",
    "        dismissed = dismiss_login_wall(driver)\n",
    "        if not dismissed:\n",
    "            print(\"Login wall present; cannot dismiss. Try logging in.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        wait_for_job_items(driver, timeout=20)\n",
    "    except TimeoutException:\n",
    "        print(\"⚠️ No job cards visible (timeout).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    container = None\n",
    "    for sel in CONTAINER_SELECTORS:\n",
    "        try:\n",
    "            container = driver.find_element(By.CSS_SELECTOR, sel); break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    for _ in range(2):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(0.4)\n",
    "    if container:\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", container); time.sleep(0.4)\n",
    "\n",
    "    items = []\n",
    "    for sel in ITEM_SELECTORS:\n",
    "        found = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if found:\n",
    "            items = found; break\n",
    "\n",
    "    if container:\n",
    "        try:\n",
    "            items = scroll_until_all_jobs_load(driver, container)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if items:\n",
    "        df = populate_jobs_data_minimal(items, nb_to_scrape)\n",
    "        if not df.empty:\n",
    "            return df\n",
    "\n",
    "    df2 = extract_from_list_cards_min(max_items=nb_to_scrape)\n",
    "    if not df2.empty:\n",
    "        return df2\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# In LinkedIn, one page contains 25 jobs by default. This function tries to navigate to the next page by clicking the \"Next\" button or a specific page number button.\n",
    "def move_to_next_page(target_page_num: Optional[int] = None, timeout: int = 12) -> bool:\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, timeout)\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(0.3)\n",
    "\n",
    "        clicked = False\n",
    "        if target_page_num is not None:\n",
    "            for xp in [\n",
    "                f'//button[normalize-space()=\"{target_page_num}\"]',\n",
    "                f'//button[.//span[normalize-space()=\"{target_page_num}\"]]'\n",
    "            ]:\n",
    "                try:\n",
    "                    btn = wait.until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "                    driver.execute_script('arguments[0].scrollIntoView({block:\"center\"});', btn); time.sleep(0.2)\n",
    "                    try:\n",
    "                        btn.click()\n",
    "                    except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    clicked = True; break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "\n",
    "        if not clicked:\n",
    "            next_btns = driver.find_elements(By.CSS_SELECTOR, 'button[aria-label*=\"Next\"], button[aria-label*=\"next\"]')\n",
    "            if not next_btns:\n",
    "                next_btns = driver.find_elements(By.XPATH, '//button[normalize-space()=\"Next\" or .//span[normalize-space()=\"Next\"]]')\n",
    "            next_btns = [b for b in next_btns if b.is_displayed() and b.is_enabled()]\n",
    "            if not next_btns:\n",
    "                return False\n",
    "            btn = next_btns[0]\n",
    "            driver.execute_script('arguments[0].scrollIntoView({block:\"center\"});', btn); time.sleep(0.2)\n",
    "            try:\n",
    "                btn.click()\n",
    "            except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"ul.jobs-search-results__list, .scaffold-layout__list, ul.jobs-search__results-list\")))\n",
    "        time.sleep(0.4)\n",
    "        if login_wall_present(driver):\n",
    "            dismiss_login_wall(driver)\n",
    "        return True\n",
    "    except (TimeoutException, StaleElementReferenceException):\n",
    "        return False\n",
    "\n",
    "\n",
    "JOBS_PER_PAGE = 25\n",
    "# Build the LinkedIn job search URL based on the provided parameters, including keywords, location, experience level, and posting time.\n",
    "def build_job_search_url(\n",
    "    keywords: str = \"Computer Science\",\n",
    "    location: str = \"United States\",\n",
    "    experience: ExperienceFilter = ExperienceFilter.ALL,\n",
    "    posted: JobPostTime = JobPostTime.ANY_TIME\n",
    ") -> str:\n",
    "    base = \"https://www.linkedin.com/jobs/search\"\n",
    "    params = []\n",
    "    if keywords:\n",
    "        params.append(f\"keywords={quote_plus(keywords)}\")\n",
    "    if location:\n",
    "        params.append(f\"location={quote_plus(location)}\")\n",
    "    if experience.value:\n",
    "        params.append(f\"f_E={experience.value}\")\n",
    "    if posted.value:\n",
    "        params.append(f\"f_TPR={posted.value}\")\n",
    "    params.append(\"position=1\"); params.append(\"pageNum=0\")\n",
    "    return f\"{base}?{'&'.join(params)}\"\n",
    "\n",
    "\n",
    "# Main function to scrape LinkedIn jobs, it could get the specified number of job postings by navigating through multiple pages if necessary.\n",
    "def scrape_linkedin_jobs(nb_jobs: int, url: str, jobs_per_page: int = None, max_total: int = 1000) -> pd.DataFrame:\n",
    "    global driver\n",
    "    if jobs_per_page is None:\n",
    "        jobs_per_page = JOBS_PER_PAGE\n",
    "    driver = ensure_window(driver)\n",
    "    driver = safe_get(driver, url)\n",
    "\n",
    "    _maybe_accept_cookies_and_check_login(driver)\n",
    "    if login_wall_present(driver):\n",
    "        dismissed = dismiss_login_wall(driver)\n",
    "        if not dismissed:\n",
    "            if EMAIL and PASSWORD:\n",
    "                try_login(driver, EMAIL, PASSWORD)\n",
    "                driver = safe_get(driver, url)\n",
    "                _maybe_accept_cookies_and_check_login(driver)\n",
    "                if login_wall_present(driver):\n",
    "                    dismiss_login_wall(driver)\n",
    "            else:\n",
    "                raise TimeoutException(\"Blocked by login wall and no credentials provided.\")\n",
    "\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "    try:\n",
    "        wait_for_job_items(driver, timeout=20)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "    total_pages_to_scrape = ceil(nb_jobs / jobs_per_page)\n",
    "    current_page = 1\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "    while True:\n",
    "        remaining = nb_jobs - len(total_df)\n",
    "        if remaining <= 0 or len(total_df) >= max_total:\n",
    "            break\n",
    "        nb_this_page = min(remaining, jobs_per_page)\n",
    "        print(f\"Scraping page {current_page} (need {nb_this_page} jobs from this page)\")\n",
    "        try:\n",
    "            page_df = scrape_jobs_from_page(nb_this_page)\n",
    "        except TimeoutException:\n",
    "            try:\n",
    "                driver.refresh()\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "                page_df = scrape_jobs_from_page(nb_this_page)\n",
    "            except Exception:\n",
    "                page_df = pd.DataFrame()\n",
    "        if not page_df.empty:\n",
    "            total_df = pd.concat([total_df, page_df], ignore_index=True)\n",
    "            if \"Link\" in total_df.columns:\n",
    "                total_df.drop_duplicates(subset=[\"Link\"], keep=\"first\", inplace=True, ignore_index=True)\n",
    "        if current_page >= total_pages_to_scrape or len(total_df) >= nb_jobs or len(total_df) >= max_total:\n",
    "            break\n",
    "        moved = move_to_next_page(current_page + 1)\n",
    "        if not moved:\n",
    "            print(\"No further pages found; stopping.\")\n",
    "            break\n",
    "        current_page += 1\n",
    "        time.sleep(0.9)\n",
    "\n",
    "    total_df.reset_index(drop=True, inplace=True)\n",
    "    return total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ebc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "_HEADING_MAP = {\n",
    "    \"responsibilities\": \"resp\",\n",
    "    \"what you will do\": \"resp\",\n",
    "    \"what you'll do\": \"resp\",\n",
    "    \"what you do\": \"resp\",\n",
    "    \"duties\": \"resp\",\n",
    "    \"key duties\": \"resp\",\n",
    "    \"role & responsibilities\": \"resp\",\n",
    "    \"role and responsibilities\": \"resp\",\n",
    "    \"requirements\": \"genreq\",\n",
    "    \"must have\": \"genreq\",\n",
    "    \"you have\": \"genreq\",\n",
    "    \"required qualifications\": \"req\",\n",
    "    \"minimum qualifications\": \"req\",\n",
    "    \"basic qualifications\": \"req\",\n",
    "    \"qualifications\": \"req\",\n",
    "    \"preferred qualifications\": \"pref\",\n",
    "    \"preferred\": \"pref\",\n",
    "    \"nice to have\": \"pref\",\n",
    "    \"bonus\": \"pref\",\n",
    "    \"plus\": \"pref\",\n",
    "}\n",
    "_heading_regex = re.compile(\n",
    "    r\"^\\s*(?:\" +\n",
    "    r\"|\".join(re.escape(k) for k in sorted(_HEADING_MAP, key=len, reverse=True)) +\n",
    "    r\")\\s*:?.*$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _normalize_bullets(text: str) -> str:\n",
    "    t = re.sub(r\"\\r\\n?\", \"\\n\", text or \"\")\n",
    "    t = re.sub(r\"[•·▪●◦–—\\-]\\s*\", \"\\n- \", t)\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def parse_description_sections(text: str):\n",
    "    text = _normalize_bullets(text)\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "    buckets = {\"resp\": [], \"req\": [], \"pref\": [], \"genreq\": []}\n",
    "    current = None\n",
    "    for ln in lines:\n",
    "        lnl = ln.lower()\n",
    "        if _heading_regex.match(lnl):\n",
    "            for key, bucket in _HEADING_MAP.items():\n",
    "                if key in lnl:\n",
    "                    current = bucket; break\n",
    "            continue\n",
    "        item = ln.lstrip(\"- \").strip()\n",
    "        if not item: continue\n",
    "        if current == \"resp\" and len(buckets[\"resp\"]) < 60: buckets[\"resp\"].append(item)\n",
    "        elif current == \"req\" and len(buckets[\"req\"]) < 60: buckets[\"req\"].append(item)\n",
    "        elif current == \"pref\" and len(buckets[\"pref\"]) < 60: buckets[\"pref\"].append(item)\n",
    "        elif current == \"genreq\" and len(buckets[\"genreq\"]) < 60: buckets[\"genreq\"].append(item)\n",
    "    if not any(buckets.values()):\n",
    "        bullets = [ln.lstrip(\"- \").strip() for ln in lines if ln.startswith(\"- \")]\n",
    "        buckets[\"req\"] = bullets[:30]\n",
    "    return buckets\n",
    "\n",
    "# Apply the parsing function to each job description in the DataFrame and create new columns for each section.\n",
    "def split_description_sections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"Responsibilities\"] = [[] for _ in range(len(out))]\n",
    "    out[\"QualificationsRequired\"] = [[] for _ in range(len(out))]\n",
    "    out[\"QualificationsPreferred\"] = [[] for _ in range(len(out))]\n",
    "    out[\"Requirements\"] = [[] for _ in range(len(out))]\n",
    "    for i, desc in enumerate(out[\"Description\"].fillna(\"\")):\n",
    "        buckets = parse_description_sections(desc)\n",
    "        out.at[i, \"Responsibilities\"] = buckets[\"resp\"]\n",
    "        out.at[i, \"QualificationsRequired\"] = buckets[\"req\"]\n",
    "        out.at[i, \"QualificationsPreferred\"] = buckets[\"pref\"]\n",
    "        out.at[i, \"Requirements\"] = buckets[\"genreq\"]\n",
    "    return out\n",
    "\n",
    "# A convenience function that combines building the search URL, scraping the jobs, and parsing the descriptions into a single DataFrame.\n",
    "def scrape_and_parse_linkedin_jobs(nb_jobs: int,\n",
    "                                   keywords: str = \"Computer Science\",\n",
    "                                   location: str = \"United States\",\n",
    "                                   experience: ExperienceFilter = ExperienceFilter.ALL,\n",
    "                                   posted: JobPostTime = JobPostTime.ANY_TIME) -> pd.DataFrame:\n",
    "    \"\"\"Returns a SINGLE DataFrame with Title, Company, Location, Link, Description and parsed columns.\"\"\"\n",
    "    url = build_job_search_url(keywords=keywords, location=location, experience=experience, posted=posted)\n",
    "    df = scrape_linkedin_jobs(nb_jobs=nb_jobs, url=url)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = split_description_sections(df)\n",
    "    # reorder columns for convenience\n",
    "    cols = [\"Title\",\"Company\",\"Location\",\"Link\",\"Description\",\n",
    "            \"Responsibilities\",\"QualificationsRequired\",\"QualificationsPreferred\",\"Requirements\"]\n",
    "    return df[[c for c in cols if c in df.columns] + [c for c in df.columns if c not in cols]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71099895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 1: Augment each scraped job with Salary, EmploymentType, and AboutTheCompany (Applicants removed) ---\n",
    "import re, time\n",
    "from pathlib import Path\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "\n",
    "_META_JOBTYPE_PATTERNS = [\n",
    "    r\"full[\\s\\-]?time\", r\"part[\\s\\-]?time\", r\"contract(?!or)\", r\"contractor\", r\"temporary\",\n",
    "    r\"intern(ship)?\", r\"freelance\", r\"seasonal\"\n",
    "]\n",
    "\n",
    "def _all_texts(drv, css_list=None, xp_list=None):\n",
    "    out = []\n",
    "    css_list = css_list or []\n",
    "    xp_list = xp_list or []\n",
    "    for sel in css_list:\n",
    "        for e in drv.find_elements(By.CSS_SELECTOR, sel):\n",
    "            t = (e.text or \"\").strip()\n",
    "            if t:\n",
    "                out.append(t)\n",
    "    for xp in xp_list:\n",
    "        for e in drv.find_elements(By.XPATH, xp):\n",
    "            t = (e.text or \"\").strip()\n",
    "            if t:\n",
    "                out.append(t)\n",
    "    return out\n",
    "\n",
    "def _click_show_more_if_present(drv):\n",
    "    variants = [\n",
    "        '//button[contains(@aria-label, \"Show more\")]',\n",
    "        '//button[.//span[contains(translate(., \"SHOWMORE\", \"showmore\"), \"show more\")]]',\n",
    "        '//button[contains(translate(., \"SHOW\", \"show\"), \"show more\")]',\n",
    "        '//button[contains(translate(., \"SEE\", \"see\"), \"see more\")]',\n",
    "        '//button[contains(@class, \"show-more-less-html__button\")]',\n",
    "        '//button[contains(@class, \"artdeco-button\") and .//span[contains(translate(., \"MORE\", \"more\"), \"more\")]]',\n",
    "    ]\n",
    "    for xp in variants:\n",
    "        try:\n",
    "            for b in drv.find_elements(By.XPATH, xp)[:4]:\n",
    "                try:\n",
    "                    drv.execute_script('arguments[0].scrollIntoView({block:\"center\"});', b); time.sleep(0.1)\n",
    "                    try:\n",
    "                        b.click()\n",
    "                    except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                        drv.execute_script(\"arguments[0].click();\", b)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _page_text(drv) -> str:\n",
    "    try:\n",
    "        return drv.execute_script(\"return document.body.innerText || '';\") or \"\"\n",
    "    except Exception:\n",
    "        try:\n",
    "            return drv.find_element(By.TAG_NAME, \"body\").text\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def _extract_employment_type(drv, txt=None):\n",
    "    texts = _all_texts(drv,\n",
    "        css_list=[\n",
    "            \".job-details__content\",\n",
    "            \".jobs-description__container\",\n",
    "            \".jobs-unified-top-card__primary-description-container\",\n",
    "            \".jobs-unified-top-card__subtitle-primary-grouping\",\n",
    "            \"section.jobs-box\",\n",
    "        ])\n",
    "    joined = \" | \".join(texts).lower()\n",
    "    for pat in _META_JOBTYPE_PATTERNS:\n",
    "        m = re.search(pat, joined, flags=re.I)\n",
    "        if m:\n",
    "            return m.group(0).replace(\"-\", \" \").title()\n",
    "    if txt:\n",
    "        for pat in _META_JOBTYPE_PATTERNS:\n",
    "            m = re.search(pat, txt, flags=re.I)\n",
    "            if m:\n",
    "                return m.group(0).replace(\"-\", \" \").title()\n",
    "    return \"\"\n",
    "\n",
    "_SAL_PATTERNS = [\n",
    "    re.compile(r\"(\\$|£|€)\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\s*(?:[KkMm])?\\s*(?:[-–—]\\s*(?:\\$|£|€)?\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\s*(?:[KkMm])?)?\\s*(?:/|per\\s+)?(year|yr|month|mo|hour|hr|week|wk|day|annum)\\b\", re.I),\n",
    "    re.compile(r\"(?:salary|pay|compensation)\\s*[:\\-]?\\s*(\\$|£|€)\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?(?:\\s*[Kk])?(?:\\s*[-–—]\\s*(?:\\$|£|€)?\\s?\\d{1,3}(?:,\\d{3})*)?\", re.I),\n",
    "    re.compile(r\"\\$?\\s?\\d{2,3}\\s?(?:-\\s?\\$?\\s?\\d{2,3})\\s?(?:per\\s+hour|/hour|hr)\\b\", re.I),\n",
    "]\n",
    "\n",
    "def _extract_salary_from_text(txt: str) -> str:\n",
    "    for pat in _SAL_PATTERNS:\n",
    "        m = pat.search(txt)\n",
    "        if m:\n",
    "            return m.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def _extract_salary(drv, txt=None):\n",
    "    containers = _all_texts(drv,\n",
    "        css_list=[\n",
    "            \".job-details__content\", \".jobs-description__container\",\n",
    "            \".jobs-unified-top-card__job-insight\", \".jobs-box__html-content\",\n",
    "            \".jobs-box--expandable-content\", \".jobs-salary\", \".jobs-pay\"\n",
    "        ])\n",
    "    for block in containers:\n",
    "        s = _extract_salary_from_text(block)\n",
    "        if s:\n",
    "            return s\n",
    "    if txt:\n",
    "        return _extract_salary_from_text(txt)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _clean_about(text: str, max_len=1500) -> str:\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\", text or \"\").strip()\n",
    "    # soft-trim boilerplate tails if they appear\n",
    "    t = re.split(r\"(?:Equal Opportunity Employer|EEO|Pay Transparency|Benefits include)[:\\s]\", t, maxsplit=1)[0].strip()\n",
    "    return t[:max_len]\n",
    "\n",
    "\n",
    "def _extract_about_company_section(drv, txt=None, max_len=1500):\n",
    "    \"\"\"\n",
    "    Robust extractor for the 'About the company' section on LinkedIn job pages.\n",
    "    Strategy (in order):\n",
    "      1) CSS container hunt for known company blocks (jobs-company / about IDs).\n",
    "      2) Header match ('About the company' / 'About company' / 'About us' / 'Company overview'), then sibling-walk.\n",
    "      3) JavaScript sibling-walk from the last matching H2/H3 heading.\n",
    "      4) Whole-page text fallback starting at the header text.\n",
    "    \"\"\"\n",
    "    def _clean_about(text: str) -> str:\n",
    "        t = re.sub(r\"\\n{2,}\", \"\\n\", text or \"\").strip()\n",
    "        # Remove obvious boilerplate tails\n",
    "        t = re.split(r\"(?:Interested in working|Members who share|Learn more\\b|I['’]m interested|Equal Opportunity Employer|EEO|Pay Transparency)[:\\s]\", t, maxsplit=1)[0].strip()\n",
    "        return t[:max_len]\n",
    "\n",
    "    # 1) Known containers: try to read the block even if heading markup changes\n",
    "    container_css = [\n",
    "        \"section.jobs-company\", \"div.jobs-company\", \"section.jobs-company__card\",\n",
    "        \"div.jobs-company__box\", \"section.jobs-box--company\",\n",
    "        \"section[data-test-id*='about']\", \"div[data-test-id*='about']\",\n",
    "        \"section#about-company\", \"div#about-company\",\n",
    "    ]\n",
    "    for sel in container_css:\n",
    "        try:\n",
    "            blocks = drv.find_elements(By.CSS_SELECTOR, sel)\n",
    "            for b in blocks[::-1]:  # prefer later blocks\n",
    "                txt_block = (b.text or \"\").strip()\n",
    "                if not txt_block:\n",
    "                    continue\n",
    "                # Only accept if it actually contains a clue word\n",
    "                if re.search(r\"\\babout\\b.*\\bcompany\\b\", txt_block, re.I) or re.search(r\"\\bfollowers\\b|^follow$\", txt_block, re.I):\n",
    "                    # Strip the header portion if present\n",
    "                    txt_block = re.sub(r\"^.*?(about\\s+the\\s+company|about\\s+company|about\\s+us|company\\s+overview)\\s*:?[\\n]+\", \"\", txt_block, flags=re.I|re.S)\n",
    "                    cleaned = _clean_about(txt_block)\n",
    "                    if len(cleaned.split()) > 5:\n",
    "                        return cleaned\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) XPath header hunt + sibling walk in Python\n",
    "    header_xps = [\n",
    "        '//h2[contains(translate(., \"ABOUT THE COMPANY\", \"about the company\"), \"about the company\")]',\n",
    "        '//h2[contains(translate(., \"ABOUT COMPANY\", \"about company\"), \"about company\")]',\n",
    "        '//h2[contains(translate(., \"ABOUT US\", \"about us\"), \"about us\")]',\n",
    "        '//h2[contains(translate(., \"COMPANY OVERVIEW\", \"company overview\"), \"company overview\")]',\n",
    "        '//h3[contains(translate(., \"ABOUT THE COMPANY\", \"about the company\"), \"about the company\")]',\n",
    "        '//h3[contains(translate(., \"ABOUT COMPANY\", \"about company\"), \"about company\")]',\n",
    "        '//h3[contains(translate(., \"ABOUT US\", \"about us\"), \"about us\")]',\n",
    "        '//h3[contains(translate(., \"COMPANY OVERVIEW\", \"company overview\"), \"company overview\")]',\n",
    "    ]\n",
    "    def _sibling_collect_from(el):\n",
    "        # Collect following siblings' visible text until next header\n",
    "        para = []\n",
    "        node = el\n",
    "        # Try within same section first\n",
    "        try:\n",
    "            sec = el.find_element(By.XPATH, \"ancestor::section\")\n",
    "        except Exception:\n",
    "            sec = None\n",
    "        if sec:\n",
    "            kids = sec.find_elements(By.XPATH, \".//*[self::p or self::li or self::div or self::span]\")\n",
    "            for k in kids:\n",
    "                try:\n",
    "                    t = (k.text or \"\").strip()\n",
    "                    if t and len(t.split()) > 4:\n",
    "                        para.append(t)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if para:\n",
    "            return \" \".join(para).strip()\n",
    "        return \"\"\n",
    "\n",
    "    for xp in header_xps:\n",
    "        try:\n",
    "            hdrs = drv.find_elements(By.XPATH, xp)\n",
    "            if hdrs:\n",
    "                text = _sibling_collect_from(hdrs[-1])\n",
    "                if text:\n",
    "                    return _clean_about(text)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3) JavaScript sibling-walk from the last matching H2/H3\n",
    "    js = r\"\"\"\n",
    "    const hdrs = Array.from(document.querySelectorAll('h2, h3')).filter(h => /about (the )?company|about us|company overview/i.test(h.innerText.trim()));\n",
    "    if (!hdrs.length) return '';\n",
    "    const h = hdrs[hdrs.length-1];\n",
    "    const section = h.closest('section') || h.parentElement;\n",
    "    if (section){\n",
    "      Array.from(section.querySelectorAll('button')).forEach(b => {\n",
    "        const t = (b.innerText || '').toLowerCase();\n",
    "        if (t.includes('show more') || t.includes('see more') || t.includes('more')) { try{ b.click(); }catch(e){} }\n",
    "      });\n",
    "    }\n",
    "    let texts = [];\n",
    "    let node = h.nextSibling;\n",
    "    const isHeaderEl = el => el && el.matches && el.matches('h2,h3');\n",
    "    while(node){\n",
    "      if (node.nodeType === Node.ELEMENT_NODE){\n",
    "        const el = node;\n",
    "        if (isHeaderEl(el)) break;\n",
    "        const t = (el.innerText || '').trim();\n",
    "        if (t) texts.push(t);\n",
    "      } else if (node.nodeType === Node.TEXT_NODE){\n",
    "        const t = (node.textContent || '').trim();\n",
    "        if (t) texts.push(t);\n",
    "      }\n",
    "      node = node.nextSibling;\n",
    "    }\n",
    "    if (texts.join(' ').trim().length < 60 && section){\n",
    "      let whole = (section.innerText || '').trim();\n",
    "      whole = whole.replace(/^[\\s\\S]*?(about (the )?company|about us|company overview)\\s*:?\\s*/i,'');\n",
    "      return whole;\n",
    "    }\n",
    "    return texts.join('\\n').trim();\n",
    "    \"\"\"\n",
    "    try:\n",
    "        seg = drv.execute_script(js)\n",
    "        if seg and len(seg.split()) > 5:\n",
    "            return _clean_about(seg)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Whole-page text fallback\n",
    "    if txt:\n",
    "        low = txt.lower()\n",
    "        for key in [\"about the company\", \"about company\", \"about us\", \"company overview\"]:\n",
    "            pos = low.find(key)\n",
    "            if pos != -1:\n",
    "                seg = txt[pos: pos + max_len*2]\n",
    "                lines = [ln.strip() for ln in seg.splitlines() if ln.strip()]\n",
    "                stop_tokens = [\"about the team\", \"job details\", \"responsibilities\", \"qualifications\", \"benefits\", \"pay\", \"salary\"]\n",
    "                for j, ln in enumerate(lines):\n",
    "                    if any(tok in ln.lower() for tok in stop_tokens) and j > 2:\n",
    "                        lines = lines[:j]; break\n",
    "                return _clean_about(\" \".join(lines))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def _robust_scroll(drv):\n",
    "    try:\n",
    "        drv.execute_script(\"window.scrollTo(0, 0);\"); time.sleep(0.1)\n",
    "        h_last = 0\n",
    "        for _ in range(12):\n",
    "            drv.execute_script(\"window.scrollBy(0, window.innerHeight*0.85);\"); time.sleep(0.2)\n",
    "            h = drv.execute_script(\"return document.documentElement.scrollTop || document.body.scrollTop || 0;\")\n",
    "            if h == h_last: break\n",
    "            h_last = h\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "_WORKPLACE_PATTERNS = [\n",
    "    r\"\\bremote\\b\", r\"\\bhybrid\\b\", r\"\\bon[\\-\\s]?site\\b\", r\"\\bonsite\\b\"\n",
    "]\n",
    "\n",
    "def _extract_workplace_type(drv, txt=None):\n",
    "    # Prefer the explicit Workplace-type badge/label in the top card\n",
    "    texts = _all_texts(drv,\n",
    "        css_list=[\n",
    "            \".jobs-unified-top-card__workplace-type\",\n",
    "            \".jobs-unified-top-card__primary-description-container li\",\n",
    "            \".jobs-unified-top-card__primary-description-container span\",\n",
    "            \".jobs-unified-top-card__subtitle-primary-grouping span\",\n",
    "            \".jobs-unified-top-card__subtitle-primary-grouping\",\n",
    "        ])\n",
    "    def norm(val: str) -> str:\n",
    "        v = val.lower()\n",
    "        if \"remote\" in v: return \"Remote\"\n",
    "        if \"hybrid\" in v: return \"Hybrid\"\n",
    "        if \"on-site\" in v or \"onsite\" in v or \"on site\" in v: return \"On-site\"\n",
    "        return \"\"\n",
    "    joined = \" | \".join(texts)\n",
    "    for pat in _WORKPLACE_PATTERNS:\n",
    "        m = re.search(pat, joined, flags=re.I)\n",
    "        if m:\n",
    "            w = norm(m.group(0))\n",
    "            if w: return w\n",
    "    if txt:\n",
    "        for pat in _WORKPLACE_PATTERNS:\n",
    "            m = re.search(pat, txt, flags=re.I)\n",
    "            if m:\n",
    "                w = norm(m.group(0))\n",
    "                if w: return w\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def augment_with_job_meta(df, per_job_delay=0.6, limit=None, dump_debug=False):\n",
    "    global driver\n",
    "    if df.empty or \"Link\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    # Ensure target columns exist\n",
    "    for col, default in [(\"Salary\",\"\"), (\"EmploymentType\",\"\"), (\"AboutTheCompany\",\"\"), (\"WorkplaceType\",\"\")]:\n",
    "        if col not in out.columns:\n",
    "            out[col] = default\n",
    "    # Optionally drop 'Applicants' if present\n",
    "    if \"Applicants\" in out.columns:\n",
    "        try:\n",
    "            out.drop(columns=[\"Applicants\"], inplace=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _n = len(out) if limit is None else min(limit, len(out))\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "\n",
    "    for i in range(_n):\n",
    "        link = out.at[i, \"Link\"]\n",
    "        if not link or not isinstance(link, str):\n",
    "            continue\n",
    "        try:\n",
    "            driver = ensure_window(driver)\n",
    "            driver = safe_get(driver, link)\n",
    "            try:\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".jobs-unified-top-card\")))\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "\n",
    "            _click_show_more_if_present(driver)\n",
    "            _robust_scroll(driver)\n",
    "            page_text = _page_text(driver)\n",
    "\n",
    "            emp = _extract_employment_type(driver, page_text)\n",
    "            sal = _extract_salary(driver, page_text)\n",
    "            about = _extract_about_company_section(driver, page_text)\n",
    "            wpt = _extract_workplace_type(driver, page_text)\n",
    "\n",
    "            if emp: out.at[i, \"EmploymentType\"] = emp\n",
    "            if sal: out.at[i, \"Salary\"] = sal\n",
    "            if about: out.at[i, \"AboutTheCompany\"] = about\n",
    "            if wpt: out.at[i, \"WorkplaceType\"] = wpt\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Meta extraction failed for row {i}: {e}\")\n",
    "        finally:\n",
    "            time.sleep(max(per_job_delay, 0.8))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 (need 10 jobs from this page)\n"
     ]
    }
   ],
   "source": [
    "driver = ensure_window(driver); driver = safe_get(driver, \"https://www.linkedin.com/jobs/\")\n",
    "\n",
    "final_df = scrape_and_parse_linkedin_jobs(\n",
    "    nb_jobs=40000,                          # Adjust the number of jobs to scrape\n",
    "    keywords=\"Computer Science\",            # Adjust the keywords to search for\n",
    "    location=\"United States\",               # Adjust the location to search in\n",
    "    experience=ExperienceFilter.ALL,\n",
    "    posted=JobPostTime.ANY_TIME             # Adjust the posting time filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "295541d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = augment_with_job_meta(final_df, per_job_delay=0.8, limit=None, dump_debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# Get correct job locations from mixed information\n",
    "parts = final_df[\"Location\"].fillna(\"\").str.replace(\"\\n\", \" · \", regex=False).str.split(\"·\")\n",
    "final_df[\"Location\"] = parts.str[0].str.strip()\n",
    "final_df[\"Applicants\"] = parts.apply(lambda xs: next(((m.group(0).strip()) for s in xs if (m:=re.search(r'(?i)(?:over\\s+)?\\d[\\d,]*\\s+(?:people\\s+(?:clicked\\s+apply|appl(?:y|ied))|applicants?)', s.strip()))), None))\n",
    "final_df['Description'] = final_df['Description'].str.replace('About the job\\n','').str.strip()\n",
    "final_df['AboutTheCompany'] = final_df['AboutTheCompany'].str.replace(r'^\\s*(?:.*\\b(?:follow|followers)\\b.*\\n)*\\s*(?:.*\\bemployees\\b.*\\bon\\s+LinkedIn\\b.*\\n)?', '', regex=True).str.strip()\n",
    "\n",
    "final_df[\"Posted_text\"] = parts.apply(\n",
    "    lambda xs: next((t.strip() for t in xs if isinstance(xs, list) and (\"ago\" in t.lower() or \"just now\" in t.lower())), \"\")\n",
    ")\n",
    "\n",
    "# The jobs posted time in LinkedIn usually looks like \"xx days/months/years ago\", so we need to use the today's time to get specific date\n",
    "def to_today_minus(s: str) -> str:\n",
    "    s = (s or \"\").lower().strip()\n",
    "    if \"just now\" in s:\n",
    "        return \"today\"\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    if m and \"day\" in s:\n",
    "        n = int(m.group(1))\n",
    "        return \"today - \" + str(n) + (\"day\" if n == 1 else \"days\")\n",
    "    return \"\"\n",
    "\n",
    "final_df[\"Posted_relative\"] = final_df[\"Posted_text\"].apply(to_today_minus)\n",
    "\n",
    "\n",
    "# This function allow us to use Chicago time to show the posted date in form of \"mm/dd/yyyy\"\n",
    "def to_posted_date(s: str, now=None, tz: str = \"America/Chicago\"):\n",
    "    if now is None:\n",
    "        now = pd.Timestamp.now(tz)  # current time with timezone\n",
    "\n",
    "    s = (str(s) or \"\").lower().strip()\n",
    "    if not s:\n",
    "        return pd.NaT\n",
    "    if \"just now\" in s:\n",
    "        return now.normalize().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    m = re.search(r\"(\\d+)\\s*(minute|hour|day|week|month|year)s?\\s*ago\", s)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "\n",
    "    n = int(m.group(1))\n",
    "    unit = m.group(2)\n",
    "\n",
    "    if unit == \"minute\":\n",
    "        dt = now - pd.Timedelta(minutes=n)\n",
    "    elif unit == \"hour\":\n",
    "        dt = now - pd.Timedelta(hours=n)\n",
    "    elif unit == \"day\":\n",
    "        dt = now - pd.Timedelta(days=n)\n",
    "    elif unit == \"week\":\n",
    "        dt = now - pd.Timedelta(weeks=n)\n",
    "    elif unit == \"month\":\n",
    "        dt = now - DateOffset(months=n)   \n",
    "    elif unit == \"year\":\n",
    "        dt = now - DateOffset(years=n)    \n",
    "    else:\n",
    "        return pd.NaT\n",
    "    return dt.normalize().strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a76a8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Posted_date\"] = final_df[\"Posted_text\"].apply(to_posted_date)\n",
    "final_df.drop(columns=[\"Posted_text\", \"Posted_relative\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3044853",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"linkedin_jobs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
