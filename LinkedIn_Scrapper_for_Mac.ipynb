{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacd2a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/50357691/Desktop/Job_Skill_Gap_Analysis/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from math import ceil\n",
    "from typing import Optional\n",
    "import os, time, logging, pandas as pd, re\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException,\n",
    "    NoSuchWindowException, WebDriverException,\n",
    "    ElementClickInterceptedException, StaleElementReferenceException\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "class ExperienceFilter(Enum):\n",
    "    ALL = \"\"\n",
    "    INTERNSHIP = \"1\"\n",
    "    ENTRY_LEVEL = \"2\"\n",
    "    ASSOCIATE = \"3\"\n",
    "    MID_SENIOR_LEVEL = \"4\"\n",
    "    DIRECTOR = \"5\"\n",
    "    EXECUTIVE = \"6\"\n",
    "\n",
    "class JobPostTime(Enum):\n",
    "    ANY_TIME = \"\"\n",
    "    PAST_24_HOURS = \"r86400\"\n",
    "    PAST_WEEK = \"r604800\"\n",
    "    PAST_MONTH = \"r2592000\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use chrome driver to search for job postinigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6a7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium is a automation tool that can control a web browser. It could do human actions in web pages like click, scroll, input text, etc. Which can be used to bypass some anti-scraping mechanisms.\n",
    "\n",
    "driver = None\n",
    "\n",
    "def make_driver(headless: bool = False):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--lang=en-US\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "def ensure_window(drv, headless: bool = False):\n",
    "    global driver\n",
    "    if drv is None:\n",
    "        driver = make_driver(headless=headless)\n",
    "        return driver\n",
    "    try:\n",
    "        _ = drv.current_url\n",
    "        if not drv.window_handles:\n",
    "            raise NoSuchWindowException(\"No window handles\")\n",
    "        return drv\n",
    "    except Exception:\n",
    "        logging.warning(\"Driver/window lost. Recreating browser...\")\n",
    "        try:\n",
    "            drv.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "        driver = make_driver(headless=headless)\n",
    "        return driver\n",
    "\n",
    "def safe_get(drv, url: str, retries: int = 1, headless: bool = False):\n",
    "    global driver\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            drv.get(url)\n",
    "            return drv\n",
    "        except WebDriverException as e:\n",
    "            logging.warning(f\"driver.get failed ({e.__class__.__name__}); recreating…\")\n",
    "            drv = ensure_window(drv, headless=headless)\n",
    "            driver = drv\n",
    "    return drv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5a46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "def try_login(drv, email: str, password: str) -> None:\n",
    "    global driver\n",
    "    driver = ensure_window(drv)\n",
    "    driver = safe_get(driver, \"https://www.linkedin.com/login\")\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"username\")))\n",
    "    u = driver.find_element(By.ID, \"username\"); u.clear(); u.send_keys(email)\n",
    "    p = driver.find_element(By.ID, \"password\"); p.clear(); p.send_keys(password); p.send_keys(Keys.RETURN)\n",
    "    WebDriverWait(driver, 25).until(\n",
    "        EC.any_of(\n",
    "            EC.presence_of_element_located((By.ID, \"global-nav-search\")),\n",
    "            EC.presence_of_element_located((By.ID, \"input__email_verification_pin\")),\n",
    "            EC.url_contains(\"checkpoint\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _maybe_accept_cookies_and_check_login(drv) -> bool:\n",
    "    for sel in [\n",
    "        \"button[aria-label*='Accept']\",\n",
    "        \"button[data-test-global-nav-cookie-banner-accept]\",\n",
    "        \".artdeco-global-alert-action button\",\n",
    "    ]:\n",
    "        try:\n",
    "            for b in drv.find_elements(By.CSS_SELECTOR, sel):\n",
    "                if b.is_displayed() and b.is_enabled():\n",
    "                    b.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "    cur = drv.current_url\n",
    "    if \"login\" in cur or \"checkpoint\" in cur:\n",
    "        return False\n",
    "    try:\n",
    "        WebDriverWait(drv, 6).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"main, body\")))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def login_wall_present(drv):\n",
    "    try:\n",
    "        modals = drv.find_elements(By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')\n",
    "        return any(m.is_displayed() for m in modals)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# LinkedIn has many different languages, so the dismiss button could be in different languages. Here we try to cover some common ones.\n",
    "def dismiss_login_wall(drv, timeout=3) -> bool:\n",
    "    selectors = [\n",
    "        'button.artdeco-modal__dismiss',\n",
    "        'button[aria-label*=\"Dismiss\"]',\n",
    "        'button[aria-label*=\"Close\"]',\n",
    "        'button[aria-label*=\"关闭\"]',\n",
    "        'button[aria-label*=\"關閉\"]',\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, timeout).until(EC.element_to_be_clickable((By.CSS_SELECTOR, sel)))\n",
    "            drv.execute_script(\"arguments[0].click();\", btn)\n",
    "            WebDriverWait(drv, 5).until_not(EC.presence_of_element_located((By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')))\n",
    "            return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        drv.find_element(By.TAG_NAME, \"body\").send_keys(Keys.ESCAPE)\n",
    "        WebDriverWait(drv, 2).until_not(EC.presence_of_element_located((By.CSS_SELECTOR, '.artdeco-modal, [role=\"dialog\"]')))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Credentials \n",
    "EMAIL = \"zh272@georgetown.edu\"\n",
    "\n",
    "# Protect your password\n",
    "PASSWORD = os.getenv(\"LI_PASSWORD\") or getpass(\"Enter your LinkedIn password (hidden): \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec530c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS selectors for job listings container and items, which help to locate job postings on the page.\n",
    "CONTAINER_SELECTORS = [\n",
    "    \"div.jobs-search-results-list\",\n",
    "    \".scaffold-layout__list > div\",\n",
    "    \"div.jobs-search-two-pane__results\",\n",
    "    \"div.jobs-search-results-list__container\",\n",
    "    \"section.two-pane-serp-page__results-list\",\n",
    "]\n",
    "\n",
    "ITEM_SELECTORS = [\n",
    "    \"li[data-occludable-job-id]\",\n",
    "    \"ul.jobs-search-results__list > li\",\n",
    "    \"li.jobs-search-results__list-item\",\n",
    "    \"div.jobs-search-results__list-item\",\n",
    "    \"ul.jobs-search__results-list li\",\n",
    "    \"div.base-card, div.job-card-container\",\n",
    "]\n",
    "\n",
    "\n",
    "def wait_for_job_items(drv, timeout=20):\n",
    "    any_sel = \", \".join(ITEM_SELECTORS)\n",
    "    WebDriverWait(drv, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, any_sel)))\n",
    "\n",
    "def safe_text(el):\n",
    "    return el.text.strip() if el else \"\"\n",
    "\n",
    "def find_one(drv, *locator, timeout=8, visibility=True):\n",
    "    cond = EC.visibility_of_element_located if visibility else EC.presence_of_element_located\n",
    "    try:\n",
    "        return WebDriverWait(drv, timeout).until(cond(locator))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_title_company_location(drv):\n",
    "    # Title\n",
    "    title = \"\"\n",
    "    for sel in [\".job-view-layout h1\", \"h1.jobs-unified-top-card__job-title\", \"h1\"]:\n",
    "        els = drv.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els and els[0].text.strip():\n",
    "            title = els[0].text.strip(); break\n",
    "\n",
    "    # Company\n",
    "    company = \"\"\n",
    "    for sel in [\n",
    "        \"[data-view-name='job-details-about-company-name-link']\",\n",
    "        \"a.jobs-unified-top-card__company-name\",\n",
    "        \".jobs-unified-top-card__company-name a\",\n",
    "        \".jobs-unified-top-card__company-name\",\n",
    "    ]:\n",
    "        els = drv.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els and els[0].text.strip():\n",
    "            company = els[0].text.strip(); break\n",
    "\n",
    "    # Location: keep Remote/Hybrid/On-site and specific city/region. If it only has country, then keep empty\n",
    "    import re\n",
    "    location = \"\"\n",
    "    sel_list = [\n",
    "        \".jobs-unified-top-card__primary-description-container li\",\n",
    "        \".jobs-unified-top-card__primary-description-container span\",\n",
    "        \".job-details-jobs-unified-top-card__primary-description-container li\",\n",
    "        \".job-details-jobs-unified-top-card__primary-description-container span\",\n",
    "        \".jobs-unified-top-card__subtitle-primary-grouping span\",\n",
    "        \".jobs-unified-top-card__subtitle-primary-grouping\",\n",
    "        \".jobs-unified-top-card__bullet\",\n",
    "        \".jobs-unified-top-card__workplace-type\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for sel in sel_list:\n",
    "        for e in drv.find_elements(By.CSS_SELECTOR, sel):\n",
    "            t = e.text.strip()\n",
    "            if t:\n",
    "                candidates.append(t)\n",
    "\n",
    "    for t in candidates:\n",
    "        low = t.lower()\n",
    "        if any(k in low for k in [\"remote\", \"hybrid\", \"on-site\", \"onsite\"]):\n",
    "            location = t; break\n",
    "\n",
    "    if not location:\n",
    "        for t in candidates:\n",
    "            if re.search(r\".+,\\s+.+\", t):  \n",
    "                location = t; break\n",
    "\n",
    "    if location and location.strip().lower() in {\"united states\", \"usa\"}:\n",
    "        location = \"\"\n",
    "\n",
    "    return title, company, location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b3149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most job descriptions are partially hidden and need to be expanded by clicking \"See more\" or similar buttons. This function tries to find and click those buttons to reveal the full job description.\n",
    "def expand_all_in(drv, root=None, max_clicks=10):\n",
    "    selectors = [\n",
    "        \"button.show-more-less-html__button\",\n",
    "        \"button[aria-label*='See more']\",\n",
    "        \"button[aria-label*='Show more']\",\n",
    "        \"button[aria-controls*='description']\",\n",
    "        \"button[aria-expanded='false']\",\n",
    "        \"button[aria-label*='显示更多']\",\n",
    "        \"button[aria-label*='顯示更多']\",\n",
    "    ]\n",
    "    did = False\n",
    "    for _ in range(max_clicks):\n",
    "        clicked_this_round = False\n",
    "        scope = root if root is not None else drv\n",
    "        for sel in selectors:\n",
    "            for b in scope.find_elements(By.CSS_SELECTOR, sel):\n",
    "                try:\n",
    "                    if not b.is_displayed() or not b.is_enabled():\n",
    "                        continue\n",
    "                    drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", b)\n",
    "                    time.sleep(0.15)\n",
    "                    drv.execute_script(\"arguments[0].click();\", b)\n",
    "                    did = True; clicked_this_round = True\n",
    "                    time.sleep(0.2)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if not clicked_this_round:\n",
    "            break\n",
    "    return did\n",
    "\n",
    "def scroll_until_all_jobs_load(drv, container, pause=0.8, max_tries=30, item_selector=\"li.jobs-search-results__list-item\"):\n",
    "    tries = 0\n",
    "    last_height = drv.execute_script(\"return arguments[0].scrollHeight\", container)\n",
    "    while tries < max_tries:\n",
    "        drv.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", container)\n",
    "        time.sleep(pause)\n",
    "        new_height = drv.execute_script(\"return arguments[0].scrollHeight\", container)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height; tries += 1\n",
    "    items = drv.find_elements(By.CSS_SELECTOR, item_selector)\n",
    "    if not items:\n",
    "        items = drv.find_elements(By.CSS_SELECTOR, \"ul.jobs-search-results__list > li, li[data-occludable-job-id], ul.jobs-search__results-list li\")\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c994c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_text(root, selectors):\n",
    "    for sel in selectors:\n",
    "        els = root.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            txt = els[0].text.strip()\n",
    "            if txt:\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "def _first_el(root, selectors):\n",
    "    for sel in selectors:\n",
    "        els = root.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if els:\n",
    "            return els[0]\n",
    "    return None\n",
    "\n",
    "def _first_attr(root, selectors, attr):\n",
    "    el = _first_el(root, selectors)\n",
    "    return el.get_attribute(attr) if el else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the jobs data from the job listings\n",
    "def populate_jobs_data_minimal(jobs, nbOfJobs):\n",
    "    job_data = {\"Title\": [], \"Company\": [], \"Location\": [], \"Link\": [], \"Description\": []}\n",
    "    totalJobs = min(nbOfJobs, len(jobs))\n",
    "    list_handle = driver.current_window_handle\n",
    "\n",
    "    for idx, job in enumerate(jobs[:totalJobs]):\n",
    "        try:\n",
    "            link_el = _first_el(job, [\"a.base-card__full-link\", \"a.job-card-container__link\", \"a[href*='/jobs/view/']\"])\n",
    "            if not link_el: continue\n",
    "            link = link_el.get_attribute(\"href\") or \"\"\n",
    "            driver.execute_script(\"window.open(arguments[0], '_blank');\", link)\n",
    "            WebDriverWait(driver, 10).until(lambda d: len(d.window_handles) > 1)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            WebDriverWait(driver, 12).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".jobs-unified-top-card, .job-view-layout, h1\")))\n",
    "            expand_all_in(driver, None, max_clicks=8)\n",
    "\n",
    "            title, company, location = get_title_company_location(driver)\n",
    "\n",
    "            desc_root = None\n",
    "            for sel in [\"#job-details\", \".jobs-description__content\", \".jobs-box__html-content\", \".show-more-less-html__markup\"]:\n",
    "                found = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "                if found: desc_root = found[0]; break\n",
    "            if desc_root: expand_all_in(driver, desc_root, max_clicks=5)\n",
    "\n",
    "            description = desc_root.text.strip() if desc_root else \"\"\n",
    "\n",
    "            job_data[\"Company\"].append(company)\n",
    "            job_data[\"Title\"].append(title)\n",
    "            job_data[\"Location\"].append(location)\n",
    "            job_data[\"Link\"].append(link)\n",
    "            job_data[\"Description\"].append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[details-minimal] job {idx} skipped: {e}\")\n",
    "        finally:\n",
    "            if len(driver.window_handles) > 1:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(list_handle)\n",
    "                if login_wall_present(driver):\n",
    "                    dismiss_login_wall(driver)\n",
    "\n",
    "    return pd.DataFrame(job_data)\n",
    "\n",
    "\n",
    "# read data from the job cards in the listing page, without visiting each job's detail page\n",
    "def extract_from_list_cards_min(max_items: int = 25) -> pd.DataFrame:\n",
    "    cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search__results-list li\")\n",
    "    if not cards:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"ul.jobs-search-results__list li\")\n",
    "    if not cards:\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"div.base-card, div.job-card-container\")\n",
    "\n",
    "    data = {\"Title\": [], \"Company\": [], \"Location\": [], \"Link\": [], \"Description\": []}\n",
    "    for c in cards[:max_items]:\n",
    "        try:\n",
    "            title = _first_text(c, [\".base-search-card__title\", \".job-card-list__title\", \"h3\"])\n",
    "            company = _first_text(c, [\n",
    "                \".base-search-card__subtitle a\", \".base-search-card__subtitle\",\n",
    "                \".job-card-container__company-name\", \".job-search-card__subtitle a\",\n",
    "                \".job-search-card__subtitle\"\n",
    "            ])\n",
    "            location = _first_text(c, [\n",
    "                \".job-search-card__location\", \"span.job-search-card__location\",\n",
    "                \"li.job-card-container__metadata-item\"\n",
    "            ])\n",
    "            link = _first_attr(c, [\"a.base-card__full-link\", \"a.job-card-container__link\", \"a[href*='/jobs/view/']\"], \"href\")\n",
    "\n",
    "            if title or company or link:\n",
    "                data[\"Title\"].append(title); data[\"Company\"].append(company); data[\"Location\"].append(location)\n",
    "                data[\"Link\"].append(link); data[\"Description\"].append(\"\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c057751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape jobs from the current page\n",
    "def scrape_jobs_from_page(nb_to_scrape: int) -> pd.DataFrame:\n",
    "    if login_wall_present(driver):\n",
    "        dismissed = dismiss_login_wall(driver)\n",
    "        if not dismissed:\n",
    "            print(\"Login wall present; cannot dismiss. Try logging in.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        wait_for_job_items(driver, timeout=20)\n",
    "    except TimeoutException:\n",
    "        print(\"⚠️ No job cards visible (timeout).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    container = None\n",
    "    for sel in CONTAINER_SELECTORS:\n",
    "        try:\n",
    "            container = driver.find_element(By.CSS_SELECTOR, sel); break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    for _ in range(2):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(0.4)\n",
    "    if container:\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", container); time.sleep(0.4)\n",
    "\n",
    "    items = []\n",
    "    for sel in ITEM_SELECTORS:\n",
    "        found = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "        if found:\n",
    "            items = found; break\n",
    "\n",
    "    if container:\n",
    "        try:\n",
    "            items = scroll_until_all_jobs_load(driver, container)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if items:\n",
    "        df = populate_jobs_data_minimal(items, nb_to_scrape)\n",
    "        if not df.empty:\n",
    "            return df\n",
    "\n",
    "    df2 = extract_from_list_cards_min(max_items=nb_to_scrape)\n",
    "    if not df2.empty:\n",
    "        return df2\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# In LinkedIn, one page contains 25 jobs by default. This function tries to navigate to the next page by clicking the \"Next\" button or a specific page number button.\n",
    "def move_to_next_page(target_page_num: Optional[int] = None, timeout: int = 12) -> bool:\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, timeout)\n",
    "        for _ in range(2):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(0.3)\n",
    "\n",
    "        clicked = False\n",
    "        if target_page_num is not None:\n",
    "            for xp in [\n",
    "                f'//button[normalize-space()=\"{target_page_num}\"]',\n",
    "                f'//button[.//span[normalize-space()=\"{target_page_num}\"]]'\n",
    "            ]:\n",
    "                try:\n",
    "                    btn = wait.until(EC.element_to_be_clickable((By.XPATH, xp)))\n",
    "                    driver.execute_script('arguments[0].scrollIntoView({block:\"center\"});', btn); time.sleep(0.2)\n",
    "                    try:\n",
    "                        btn.click()\n",
    "                    except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    clicked = True; break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "\n",
    "        if not clicked:\n",
    "            next_btns = driver.find_elements(By.CSS_SELECTOR, 'button[aria-label*=\"Next\"], button[aria-label*=\"next\"]')\n",
    "            if not next_btns:\n",
    "                next_btns = driver.find_elements(By.XPATH, '//button[normalize-space()=\"Next\" or .//span[normalize-space()=\"Next\"]]')\n",
    "            next_btns = [b for b in next_btns if b.is_displayed() and b.is_enabled()]\n",
    "            if not next_btns:\n",
    "                return False\n",
    "            btn = next_btns[0]\n",
    "            driver.execute_script('arguments[0].scrollIntoView({block:\"center\"});', btn); time.sleep(0.2)\n",
    "            try:\n",
    "                btn.click()\n",
    "            except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"ul.jobs-search-results__list, .scaffold-layout__list, ul.jobs-search__results-list\")))\n",
    "        time.sleep(0.4)\n",
    "        if login_wall_present(driver):\n",
    "            dismiss_login_wall(driver)\n",
    "        return True\n",
    "    except (TimeoutException, StaleElementReferenceException):\n",
    "        return False\n",
    "\n",
    "\n",
    "JOBS_PER_PAGE = 25\n",
    "# Build the LinkedIn job search URL based on the provided parameters, including keywords, location, experience level, and posting time.\n",
    "def build_job_search_url(\n",
    "    keywords: str = \"Computer Science\",\n",
    "    location: str = \"United States\",\n",
    "    experience: ExperienceFilter = ExperienceFilter.ALL,\n",
    "    posted: JobPostTime = JobPostTime.ANY_TIME\n",
    ") -> str:\n",
    "    base = \"https://www.linkedin.com/jobs/search\"\n",
    "    params = []\n",
    "    if keywords:\n",
    "        params.append(f\"keywords={quote_plus(keywords)}\")\n",
    "    if location:\n",
    "        params.append(f\"location={quote_plus(location)}\")\n",
    "    if experience.value:\n",
    "        params.append(f\"f_E={experience.value}\")\n",
    "    if posted.value:\n",
    "        params.append(f\"f_TPR={posted.value}\")\n",
    "    params.append(\"position=1\"); params.append(\"pageNum=0\")\n",
    "    return f\"{base}?{'&'.join(params)}\"\n",
    "\n",
    "\n",
    "# Main function to scrape LinkedIn jobs, it could get the specified number of job postings by navigating through multiple pages if necessary.\n",
    "def scrape_linkedin_jobs(nb_jobs: int, url: str, jobs_per_page: int = None, max_total: int = 1000) -> pd.DataFrame:\n",
    "    global driver\n",
    "    if jobs_per_page is None:\n",
    "        jobs_per_page = JOBS_PER_PAGE\n",
    "    driver = ensure_window(driver)\n",
    "    driver = safe_get(driver, url)\n",
    "\n",
    "    _maybe_accept_cookies_and_check_login(driver)\n",
    "    if login_wall_present(driver):\n",
    "        dismissed = dismiss_login_wall(driver)\n",
    "        if not dismissed:\n",
    "            if EMAIL and PASSWORD:\n",
    "                try_login(driver, EMAIL, PASSWORD)\n",
    "                driver = safe_get(driver, url)\n",
    "                _maybe_accept_cookies_and_check_login(driver)\n",
    "                if login_wall_present(driver):\n",
    "                    dismiss_login_wall(driver)\n",
    "            else:\n",
    "                raise TimeoutException(\"Blocked by login wall and no credentials provided.\")\n",
    "\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "    try:\n",
    "        wait_for_job_items(driver, timeout=20)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "\n",
    "    total_pages_to_scrape = ceil(nb_jobs / jobs_per_page)\n",
    "    current_page = 1\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "    while True:\n",
    "        remaining = nb_jobs - len(total_df)\n",
    "        if remaining <= 0 or len(total_df) >= max_total:\n",
    "            break\n",
    "        nb_this_page = min(remaining, jobs_per_page)\n",
    "        print(f\"Scraping page {current_page} (need {nb_this_page} jobs from this page)\")\n",
    "        try:\n",
    "            page_df = scrape_jobs_from_page(nb_this_page)\n",
    "        except TimeoutException:\n",
    "            try:\n",
    "                driver.refresh()\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body\")))\n",
    "                page_df = scrape_jobs_from_page(nb_this_page)\n",
    "            except Exception:\n",
    "                page_df = pd.DataFrame()\n",
    "        if not page_df.empty:\n",
    "            total_df = pd.concat([total_df, page_df], ignore_index=True)\n",
    "            if \"Link\" in total_df.columns:\n",
    "                total_df.drop_duplicates(subset=[\"Link\"], keep=\"first\", inplace=True, ignore_index=True)\n",
    "        if current_page >= total_pages_to_scrape or len(total_df) >= nb_jobs or len(total_df) >= max_total:\n",
    "            break\n",
    "        moved = move_to_next_page(current_page + 1)\n",
    "        if not moved:\n",
    "            print(\"No further pages found; stopping.\")\n",
    "            break\n",
    "        current_page += 1\n",
    "        time.sleep(0.9)\n",
    "\n",
    "    total_df.reset_index(drop=True, inplace=True)\n",
    "    return total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "_HEADING_MAP = {\n",
    "    \"responsibilities\": \"resp\",\n",
    "    \"what you will do\": \"resp\",\n",
    "    \"what you'll do\": \"resp\",\n",
    "    \"what you do\": \"resp\",\n",
    "    \"duties\": \"resp\",\n",
    "    \"key duties\": \"resp\",\n",
    "    \"role & responsibilities\": \"resp\",\n",
    "    \"role and responsibilities\": \"resp\",\n",
    "    \"requirements\": \"genreq\",\n",
    "    \"must have\": \"genreq\",\n",
    "    \"you have\": \"genreq\",\n",
    "    \"required qualifications\": \"req\",\n",
    "    \"minimum qualifications\": \"req\",\n",
    "    \"basic qualifications\": \"req\",\n",
    "    \"qualifications\": \"req\",\n",
    "    \"preferred qualifications\": \"pref\",\n",
    "    \"preferred\": \"pref\",\n",
    "    \"nice to have\": \"pref\",\n",
    "    \"bonus\": \"pref\",\n",
    "    \"plus\": \"pref\",\n",
    "}\n",
    "_heading_regex = re.compile(\n",
    "    r\"^\\s*(?:\" +\n",
    "    r\"|\".join(re.escape(k) for k in sorted(_HEADING_MAP, key=len, reverse=True)) +\n",
    "    r\")\\s*:?.*$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _normalize_bullets(text: str) -> str:\n",
    "    t = re.sub(r\"\\r\\n?\", \"\\n\", text or \"\")\n",
    "    t = re.sub(r\"[•·▪●◦–—\\-]\\s*\", \"\\n- \", t)\n",
    "    t = re.sub(r\"\\n{2,}\", \"\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def parse_description_sections(text: str):\n",
    "    text = _normalize_bullets(text)\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "    buckets = {\"resp\": [], \"req\": [], \"pref\": [], \"genreq\": []}\n",
    "    current = None\n",
    "    for ln in lines:\n",
    "        lnl = ln.lower()\n",
    "        if _heading_regex.match(lnl):\n",
    "            for key, bucket in _HEADING_MAP.items():\n",
    "                if key in lnl:\n",
    "                    current = bucket; break\n",
    "            continue\n",
    "        item = ln.lstrip(\"- \").strip()\n",
    "        if not item: continue\n",
    "        if current == \"resp\" and len(buckets[\"resp\"]) < 60: buckets[\"resp\"].append(item)\n",
    "        elif current == \"req\" and len(buckets[\"req\"]) < 60: buckets[\"req\"].append(item)\n",
    "        elif current == \"pref\" and len(buckets[\"pref\"]) < 60: buckets[\"pref\"].append(item)\n",
    "        elif current == \"genreq\" and len(buckets[\"genreq\"]) < 60: buckets[\"genreq\"].append(item)\n",
    "    if not any(buckets.values()):\n",
    "        bullets = [ln.lstrip(\"- \").strip() for ln in lines if ln.startswith(\"- \")]\n",
    "        buckets[\"req\"] = bullets[:30]\n",
    "    return buckets\n",
    "\n",
    "# Apply the parsing function to each job description in the DataFrame and create new columns for each section.\n",
    "def split_description_sections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"Responsibilities\"] = [[] for _ in range(len(out))]\n",
    "    out[\"QualificationsRequired\"] = [[] for _ in range(len(out))]\n",
    "    out[\"QualificationsPreferred\"] = [[] for _ in range(len(out))]\n",
    "    out[\"Requirements\"] = [[] for _ in range(len(out))]\n",
    "    for i, desc in enumerate(out[\"Description\"].fillna(\"\")):\n",
    "        buckets = parse_description_sections(desc)\n",
    "        out.at[i, \"Responsibilities\"] = buckets[\"resp\"]\n",
    "        out.at[i, \"QualificationsRequired\"] = buckets[\"req\"]\n",
    "        out.at[i, \"QualificationsPreferred\"] = buckets[\"pref\"]\n",
    "        out.at[i, \"Requirements\"] = buckets[\"genreq\"]\n",
    "    return out\n",
    "\n",
    "# A convenience function that combines building the search URL, scraping the jobs, and parsing the descriptions into a single DataFrame.\n",
    "def scrape_and_parse_linkedin_jobs(nb_jobs: int,\n",
    "                                   keywords: str = \"Computer Science\",\n",
    "                                   location: str = \"United States\",\n",
    "                                   experience: ExperienceFilter = ExperienceFilter.ALL,\n",
    "                                   posted: JobPostTime = JobPostTime.ANY_TIME) -> pd.DataFrame:\n",
    "    \"\"\"Returns a SINGLE DataFrame with Title, Company, Location, Link, Description and parsed columns.\"\"\"\n",
    "    url = build_job_search_url(keywords=keywords, location=location, experience=experience, posted=posted)\n",
    "    df = scrape_linkedin_jobs(nb_jobs=nb_jobs, url=url)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = split_description_sections(df)\n",
    "    # reorder columns for convenience\n",
    "    cols = [\"Title\",\"Company\",\"Location\",\"Link\",\"Description\",\n",
    "            \"Responsibilities\",\"QualificationsRequired\",\"QualificationsPreferred\",\"Requirements\"]\n",
    "    return df[[c for c in cols if c in df.columns] + [c for c in df.columns if c not in cols]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fe5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 (need 10 jobs from this page)\n"
     ]
    }
   ],
   "source": [
    "driver = ensure_window(driver); driver = safe_get(driver, \"https://www.linkedin.com/jobs/\")\n",
    "\n",
    "final_df = scrape_and_parse_linkedin_jobs(\n",
    "    nb_jobs=10,                             # Adjust the number of jobs to scrape\n",
    "    keywords=\"Computer Science\",            # Adjust the keywords to search for\n",
    "    location=\"United States\",               # Adjust the location to search in\n",
    "    experience=ExperienceFilter.ALL,\n",
    "    posted=JobPostTime.ANY_TIME             # Adjust the posting time filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# Get correct job locations from mixed information\n",
    "parts = final_df[\"Location\"].fillna(\"\").str.replace(\"\\n\", \" · \", regex=False).str.split(\"·\")\n",
    "final_df[\"Location\"] = parts.str[0].str.strip()\n",
    "final_df[\"Posted_text\"] = parts.apply(\n",
    "    lambda xs: next((t.strip() for t in xs if isinstance(xs, list) and (\"ago\" in t.lower() or \"just now\" in t.lower())), \"\")\n",
    ")\n",
    "\n",
    "# The jobs posted time in LinkedIn usually looks like \"xx days/months/years ago\", so we need to use the today's time to get specific date\n",
    "def to_today_minus(s: str) -> str:\n",
    "    s = (s or \"\").lower().strip()\n",
    "    if \"just now\" in s:\n",
    "        return \"today\"\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    if m and \"day\" in s:\n",
    "        n = int(m.group(1))\n",
    "        return \"today - \" + str(n) + (\"day\" if n == 1 else \"days\")\n",
    "    return \"\"\n",
    "\n",
    "final_df[\"Posted_relative\"] = final_df[\"Posted_text\"].apply(to_today_minus)\n",
    "\n",
    "\n",
    "# This function allow us to use Chicago time to show the posted date in form of \"mm/dd/yyyy\"\n",
    "def to_posted_date(s: str, now=None, tz: str = \"America/Chicago\"):\n",
    "    if now is None:\n",
    "        now = pd.Timestamp.now(tz)  # current time with timezone\n",
    "\n",
    "    s = (str(s) or \"\").lower().strip()\n",
    "    if not s:\n",
    "        return pd.NaT\n",
    "    if \"just now\" in s:\n",
    "        return now.normalize().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    m = re.search(r\"(\\d+)\\s*(minute|hour|day|week|month|year)s?\\s*ago\", s)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "\n",
    "    n = int(m.group(1))\n",
    "    unit = m.group(2)\n",
    "\n",
    "    if unit == \"minute\":\n",
    "        dt = now - pd.Timedelta(minutes=n)\n",
    "    elif unit == \"hour\":\n",
    "        dt = now - pd.Timedelta(hours=n)\n",
    "    elif unit == \"day\":\n",
    "        dt = now - pd.Timedelta(days=n)\n",
    "    elif unit == \"week\":\n",
    "        dt = now - pd.Timedelta(weeks=n)\n",
    "    elif unit == \"month\":\n",
    "        dt = now - DateOffset(months=n)   \n",
    "    elif unit == \"year\":\n",
    "        dt = now - DateOffset(years=n)    \n",
    "    else:\n",
    "        return pd.NaT\n",
    "    return dt.normalize().strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Posted_date\"] = final_df[\"Posted_text\"].apply(to_posted_date)\n",
    "final_df.drop(columns=[\"Posted_text\", \"Posted_relative\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Link</th>\n",
       "      <th>Description</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>QualificationsRequired</th>\n",
       "      <th>QualificationsPreferred</th>\n",
       "      <th>Requirements</th>\n",
       "      <th>Posted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst -- Entry Level</td>\n",
       "      <td>CGI</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4290918685/...</td>\n",
       "      <td>About the job\\nPosition Description\\n\\nLaunch ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[What You’ll Bring, Bachelor’s degree from an ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Intern</td>\n",
       "      <td>Hewlett Packard Enterprise</td>\n",
       "      <td>Spring, TX</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4289266480/...</td>\n",
       "      <td>About the job\\nThis role has been designed as ...</td>\n",
       "      <td>[Management Level Definition:, Support senior ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information Security Intern</td>\n",
       "      <td>SoTalent</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4290763403/...</td>\n",
       "      <td>About the job\\nJob Title : Securities Research...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Coursework or exposure to cybersecurity / dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Intern</td>\n",
       "      <td>Hireshire</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4291119840/...</td>\n",
       "      <td>About the job\\nAbout HireShire\\n\\nHireShire is...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Knowledge of BI tools (Power BI / Tableau / L...</td>\n",
       "      <td>[Pursuing (or recently completed) B.Tech/BE/M....</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science Intern - Summer 2026</td>\n",
       "      <td>Altruist</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4279910921/...</td>\n",
       "      <td>About the job\\nAbout Altruist\\n\\nAltruist is t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[clearing brokerage firm with intuitive softwa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Information Security Analyst Intern</td>\n",
       "      <td>TrueNorth Companies, L.C.</td>\n",
       "      <td>Cedar Rapids, IA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4280490167/...</td>\n",
       "      <td>About the job\\nAre you looking for an opportun...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[week paid program, running May, August 2026. ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Paramount Pictures</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4279650100/...</td>\n",
       "      <td>About the job\\n#WeAreParamount on a mission to...</td>\n",
       "      <td>[Lead all aspects of the collection, cleaning,...</td>\n",
       "      <td>[Proficiency in data analysis tools and softwa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025/08/26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title                     Company  \\\n",
       "0          Data Analyst -- Entry Level                         CGI   \n",
       "1                          Tech Intern  Hewlett Packard Enterprise   \n",
       "2          Information Security Intern                    SoTalent   \n",
       "3                  Data Science Intern                   Hireshire   \n",
       "4    Data Science Intern - Summer 2026                    Altruist   \n",
       "5  Information Security Analyst Intern   TrueNorth Companies, L.C.   \n",
       "6                         Data Analyst          Paramount Pictures   \n",
       "\n",
       "           Location                                               Link  \\\n",
       "0        Dallas, TX  https://www.linkedin.com/jobs/view/4290918685/...   \n",
       "1        Spring, TX  https://www.linkedin.com/jobs/view/4289266480/...   \n",
       "2        Dallas, TX  https://www.linkedin.com/jobs/view/4290763403/...   \n",
       "3                    https://www.linkedin.com/jobs/view/4291119840/...   \n",
       "4   Los Angeles, CA  https://www.linkedin.com/jobs/view/4279910921/...   \n",
       "5  Cedar Rapids, IA  https://www.linkedin.com/jobs/view/4280490167/...   \n",
       "6   Los Angeles, CA  https://www.linkedin.com/jobs/view/4279650100/...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  About the job\\nPosition Description\\n\\nLaunch ...   \n",
       "1  About the job\\nThis role has been designed as ...   \n",
       "2  About the job\\nJob Title : Securities Research...   \n",
       "3  About the job\\nAbout HireShire\\n\\nHireShire is...   \n",
       "4  About the job\\nAbout Altruist\\n\\nAltruist is t...   \n",
       "5  About the job\\nAre you looking for an opportun...   \n",
       "6  About the job\\n#WeAreParamount on a mission to...   \n",
       "\n",
       "                                    Responsibilities  \\\n",
       "0                                                 []   \n",
       "1  [Management Level Definition:, Support senior ...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "5                                                 []   \n",
       "6  [Lead all aspects of the collection, cleaning,...   \n",
       "\n",
       "                              QualificationsRequired  \\\n",
       "0  [What You’ll Bring, Bachelor’s degree from an ...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [clearing brokerage firm with intuitive softwa...   \n",
       "5  [week paid program, running May, August 2026. ...   \n",
       "6  [Proficiency in data analysis tools and softwa...   \n",
       "\n",
       "                             QualificationsPreferred  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [Coursework or exposure to cybersecurity / dat...   \n",
       "3  [Knowledge of BI tools (Power BI / Tableau / L...   \n",
       "4                                                 []   \n",
       "5                                                 []   \n",
       "6                                                 []   \n",
       "\n",
       "                                        Requirements Posted_date  \n",
       "0                                                 []  2025/08/26  \n",
       "1                                                 []  2025/08/26  \n",
       "2                                                 []  2025/08/28  \n",
       "3  [Pursuing (or recently completed) B.Tech/BE/M....         NaT  \n",
       "4                                                 []  2025/08/26  \n",
       "5                                                 []  2025/08/27  \n",
       "6                                                 []  2025/08/26  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"linkedin_jobs_demo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
